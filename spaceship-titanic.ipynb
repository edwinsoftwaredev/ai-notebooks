{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNrUQfP9xwjbIwxubvM23Lc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/edwinsoftwaredev/ai-notebooks/blob/main/spaceship-titanic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "id": "iE3lX8nLbn4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"ray[tune]\""
      ],
      "metadata": {
        "id": "qFK7BAqtBbWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cx5IFOoNVH75"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tempfile\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n",
        "wandb.login()"
      ],
      "metadata": {
        "id": "rpDlNuBMb3rK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ray import train, tune\n",
        "from ray.tune.schedulers import ASHAScheduler"
      ],
      "metadata": {
        "id": "hTc8t590Buom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "X5uu5CHcYsX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle competitions download -c spaceship-titanic\n",
        "!unzip spaceship-titanic.zip"
      ],
      "metadata": {
        "id": "mxiNBO7kY43L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('train.csv')\n",
        "\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "r9d5UgFPaPAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "train_ids = df['PassengerId'].values\n",
        "\n",
        "# removes PassengerId and Name\n",
        "df = df.drop('PassengerId', axis=1)\n",
        "df = df.drop('Name', axis=1)\n",
        "\n",
        "for label,ser in df.items():\n",
        "    if ser.dtype == 'object':\n",
        "        df[label] = pd.factorize(ser)[0]\n",
        "\n",
        "    else:\n",
        "        df[label] = ser.fillna(0)\n",
        "\n",
        "\n",
        "X = df.drop('Transported', axis=1).values\n",
        "y = df['Transported'].values\n",
        "\n",
        "train_X = X[:6000]\n",
        "train_y = y[:6000]\n",
        "\n",
        "test_X = X[6000:]\n",
        "test_y = y[6000:]\n",
        "\n",
        "train_X = torch.tensor(train_X, dtype=torch.float32)\n",
        "train_y = torch.tensor(train_y, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "test_X = torch.tensor(test_X, dtype=torch.float32)\n",
        "test_y = torch.tensor(test_y, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "train_dataset = TensorDataset(train_X, train_y)\n",
        "test_dataset = TensorDataset(test_X, test_y)"
      ],
      "metadata": {
        "id": "kbsEDPIoY8gR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NN(nn.Module):\n",
        "    def __init__(self, l1_units, l2_units, l3_units, l1_dropout, l2_dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.relu_stack = nn.Sequential(\n",
        "            nn.Dropout(l1_dropout),\n",
        "\n",
        "            nn.Linear(11, l1_units, bias=False),\n",
        "            nn.BatchNorm1d(l1_units),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Dropout(l2_dropout),\n",
        "\n",
        "            nn.Linear(l1_units, l2_units, bias=False),\n",
        "            nn.BatchNorm1d(l2_units),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Linear(l2_units, l3_units, bias=False),\n",
        "            nn.BatchNorm1d(l3_units),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Linear(l3_units, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.relu_stack(x)\n"
      ],
      "metadata": {
        "id": "Vow1wsihvgia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataloader: DataLoader, batch_size, model: NN, loss_fn, optimizer, lr):\n",
        "    size = len(dataloader.dataset)\n",
        "\n",
        "    correct = 0\n",
        "    avg_loss = 0\n",
        "\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        output = model(X)\n",
        "        loss = loss_fn(output, y)\n",
        "\n",
        "        # backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        probs = torch.sigmoid(output)\n",
        "        pred = (probs >= 0.5).float()\n",
        "        cur_correct = (pred == y).sum().item()\n",
        "\n",
        "        wandb.log({\"accuracy\": cur_correct / len(X), \"loss\": loss.item()})\n",
        "\n",
        "        correct += cur_correct\n",
        "        avg_loss += loss.item()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * batch_size + len(X)\n",
        "            print(f'loss: {loss:>7f} [{current:>5d} /{size:>5d}]')\n",
        "\n",
        "    correct /= len(dataloader.dataset)\n",
        "    avg_loss /= len(dataloader)\n",
        "\n",
        "    metrics = { \"train_model_accuracy\": correct, \"train_model_loss\": avg_loss, 'learning_rate': lr }\n",
        "\n",
        "    wandb.log(metrics)\n",
        "\n",
        "    return metrics\n"
      ],
      "metadata": {
        "id": "Cq6jsEFN7Mbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(dataloader: DataLoader, model, loss_fn, lr):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "\n",
        "    test_loss, correct = 0, 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            probs = torch.sigmoid(model(X))\n",
        "            pred = (probs >= 0.5).float()\n",
        "            cur_loss = loss_fn(pred, y).item()\n",
        "            cur_correct = (pred == y).sum().item()\n",
        "\n",
        "            wandb.log({\"test_accuracy\": cur_correct / len(X), \"test_loss\": cur_loss})\n",
        "\n",
        "            test_loss += cur_loss\n",
        "            correct += cur_correct\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "\n",
        "    metrics = { \"test_model_accuracy\": correct, \"test_model_loss\": test_loss, 'learning_rate':  lr }\n",
        "\n",
        "    wandb.log(metrics)\n",
        "\n",
        "    print(f'Test Error: \\n Accuracy: {100*correct:0.1f}%, Avg loss: {test_loss:>8f} \\n')\n",
        "\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "rleGdpsk_NWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_v = pd.read_csv('test.csv')\n",
        "\n",
        "ids = df_v.PassengerId.values\n",
        "\n",
        "df_v = df_v.drop('PassengerId', axis=1)\n",
        "df_v = df_v.drop('Name', axis=1)\n",
        "\n",
        "for label,ser in df_v.items():\n",
        "    if ser.dtype == 'object':\n",
        "        df_v[label] = pd.factorize(ser)[0]\n",
        "\n",
        "    else:\n",
        "        df_v[label] = ser.fillna(0)\n",
        "\n",
        "X_val = df_v.values\n",
        "\n",
        "X_val = torch.tensor(X_val, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "hB1kIM0Fw-5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_best_model(best_result):\n",
        "    best_model = NN(\n",
        "        best_result.config['l1'],\n",
        "        best_result.config['l2'],\n",
        "        best_result.config['l3'],\n",
        "        best_result.config['l1_dropout'],\n",
        "        best_result.config['l2_dropout']\n",
        "    )\n",
        "    checkpoint_path = os.path.join(best_result.checkpoint.to_directory(), \"checkpoint.pt\")\n",
        "    model_state, _optimizer_state = torch.load(checkpoint_path)\n",
        "    best_model.load_state_dict(model_state)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        probs = torch.sigmoid(best_model(X_val))\n",
        "        preds = (probs >= 0.5).bool()\n",
        "\n",
        "    submission = [['PassengerId','Transported']]\n",
        "    for i,id in enumerate(ids):\n",
        "        submission.append([id, preds[i].item()])\n",
        "\n",
        "\n",
        "    np.savetxt('submission.csv', submission, delimiter=',', fmt='%s')\n"
      ],
      "metadata": {
        "id": "qAbY60K6o9U8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_tuned_model(config):\n",
        "    model = NN(config['l1'], config['l2'], config['l3'], config['l1_dropout'], config['l2_dropout'])\n",
        "    learning_rate = config['lr']\n",
        "    batch_size = config['batch_size']\n",
        "    epochs = config['epochs']\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    size = len(train_dataloader.dataset)\n",
        "\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    if tune.get_checkpoint():\n",
        "        loaded_checkpoint = tune.get_checkpoint()\n",
        "        with loaded_checkpoint.as_directory() as loaded_checkpoint_dir:\n",
        "            model_state, optimizer_state = torch.load(\n",
        "                os.path.join(loaded_checkpoint_dir, 'checkpoint.pt')\n",
        "            )\n",
        "\n",
        "            model.load_state_dict(model_state)\n",
        "            optimizer.load_state_dict(optimizer_state)\n",
        "\n",
        "\n",
        "    wandb.init(project='Spaceship Titanic', config={\n",
        "        \"epochs\": epochs,\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"l1_units\": config['l1'],\n",
        "        \"l2_units\": config['l2'],\n",
        "        \"l3_units\": config['l3'],\n",
        "        \"l1_dropout\": config['l1_dropout'],\n",
        "        \"l2_dropout\": config['l2_dropout']\n",
        "    })\n",
        "\n",
        "    train_metrics = None\n",
        "    test_metrics = None\n",
        "\n",
        "    for t in range(epochs):\n",
        "        print(f'Epoch {t+1}\\n---------------------')\n",
        "        train_metrics = train(train_dataloader, batch_size, model, loss_fn, optimizer, learning_rate)\n",
        "        print()\n",
        "        test_metrics = test(test_dataloader, model, loss_fn, learning_rate)\n",
        "        print()\n",
        "\n",
        "\n",
        "    wandb.finish()\n",
        "\n",
        "    with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n",
        "        path = os.path.join(temp_checkpoint_dir, 'checkpoint.pt')\n",
        "        torch.save(\n",
        "            (model.state_dict(), optimizer.state_dict()),\n",
        "            path\n",
        "        )\n",
        "\n",
        "        checkpoint = tune.Checkpoint.from_directory(temp_checkpoint_dir)\n",
        "        tune.report({**train_metrics, **test_metrics}, checkpoint=checkpoint)\n",
        "\n",
        "\n",
        "    print('Done!')\n"
      ],
      "metadata": {
        "id": "EtS9l_2yIdZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    'l1': tune.choice([256,512,1024]),\n",
        "    'l2': tune.choice([64,128,256]),\n",
        "    'l3': tune.choice([8,16,32,64]),\n",
        "    'l1_dropout': tune.uniform(0, 0.15),\n",
        "    'l2_dropout': tune.uniform(0, 0.01),\n",
        "    'lr': tune.loguniform(1e-4, 1e-2),\n",
        "    'batch_size': tune.choice([32,64]),\n",
        "    'epochs': 50,\n",
        "    'num_trials': 200\n",
        "}\n",
        "\n",
        "scheduler = ASHAScheduler(\n",
        "    time_attr=\"training_iteration\",\n",
        "    max_t=config['epochs'],\n",
        "    grace_period=20,\n",
        "    reduction_factor=2\n",
        ")\n",
        "\n",
        "tuner = tune.Tuner(\n",
        "    tune.with_resources(\n",
        "        tune.with_parameters(train_tuned_model),\n",
        "        resources={}\n",
        "    ),\n",
        "\n",
        "    tune_config=tune.TuneConfig(\n",
        "        metric='test_model_loss',\n",
        "        mode='min',\n",
        "        scheduler=scheduler,\n",
        "        num_samples=config['num_trials']\n",
        "    ),\n",
        "\n",
        "    param_space=config\n",
        ")\n",
        "\n",
        "results = tuner.fit()\n",
        "\n",
        "best_result = results.get_best_result('test_model_loss', 'min')\n",
        "\n",
        "print(f'Best trial config: { best_result.config }')\n",
        "print(f'Best trial final validation loss: { best_result.metrics[\"test_model_loss\"] }')\n",
        "print(f'Best trial final validation accuracy: { best_result.metrics[\"test_model_accuracy\"] }')\n",
        "\n",
        "test_best_model(best_result)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "8bjrWTN6gHoZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}