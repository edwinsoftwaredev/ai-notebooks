{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNYjLKQXjD2Daci/vtjOrS+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/edwinsoftwaredev/ai-notebooks/blob/main/digit_recognizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_frVBUvoDYHq"
      },
      "outputs": [],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"ray[tune]\""
      ],
      "metadata": {
        "id": "RY8VYtGTEV1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tempfile\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "ekliQi7jEfb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n",
        "wandb.login()"
      ],
      "metadata": {
        "id": "Nn8oUwT8E-Sz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ray\n",
        "from ray import train, tune\n",
        "from ray.tune.schedulers import ASHAScheduler"
      ],
      "metadata": {
        "id": "SIYvWcTfFTlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "N1uKfzvZFerv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle competitions download -c digit-recognizer\n",
        "!unzip digit-recognizer.zip"
      ],
      "metadata": {
        "id": "_9NZTvLaF0Rb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('train.csv')\n",
        "\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "1dJ8UkthF6iW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "y = df['label'].values\n",
        "X = df.drop('label',axis=1).values"
      ],
      "metadata": {
        "id": "KEABEz9zXQJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_X = X[:int(len(X)*0.8)]\n",
        "train_y = y[:int(len(X)*0.8)]\n",
        "\n",
        "test_X = X[int(len(X)*0.8):]\n",
        "test_y = y[int(len(X)*0.8):]\n",
        "\n",
        "train_X = torch.tensor(train_X, dtype=torch.float32)\n",
        "train_y = torch.tensor(train_y, dtype=torch.long)\n",
        "\n",
        "test_X = torch.tensor(test_X, dtype=torch.float32)\n",
        "test_y = torch.tensor(test_y, dtype=torch.long)\n",
        "\n",
        "train_dataset = TensorDataset(train_X, train_y)\n",
        "test_dataset = TensorDataset(test_X, test_y)\n",
        "\n",
        "train_dataset_ref = ray.put(train_dataset)\n",
        "test_dataset_ref = ray.put(test_dataset)"
      ],
      "metadata": {
        "id": "sWfjrVs_dxuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sub = pd.read_csv('test.csv')\n",
        "df_sub = df_sub.values\n",
        "submission_dataset = torch.tensor(df_sub, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "W8QqtRUoZfbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NN(nn.Module):\n",
        "    def __init__(self, l1_units, l2_units, l1_dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.relu_stack = nn.Sequential(\n",
        "            nn.Linear(784, l1_units),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Dropout(l1_dropout),\n",
        "\n",
        "            nn.Linear(l1_units, l2_units),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Linear(l2_units, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.relu_stack(x)\n"
      ],
      "metadata": {
        "id": "KjijXKgPvH0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def raytune_load_checkpoint(model: NN, optimizer: torch.optim.Adam):\n",
        "    if tune.get_checkpoint():\n",
        "        loaded_checkpoint = tune.get_checkpoint()\n",
        "        with loaded_checkpoint.as_directory() as loaded_checkpoint_dir:\n",
        "            model_state, optimizer_state = torch.load(\n",
        "                os.path.join(loaded_checkpoint_dir, 'checkpoint.pt')\n",
        "            )\n",
        "\n",
        "            model.load_state_dict(model_state)\n",
        "            optimizer.load_state_dict(optimizer_state)\n"
      ],
      "metadata": {
        "id": "bymNwIwlSTnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def raytune_save_checkpoint(model: NN, optimizer: torch.optim.Adam, train_metrics, test_metrics):\n",
        "    with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n",
        "        path = os.path.join(temp_checkpoint_dir, 'checkpoint.pt')\n",
        "        torch.save((model.state_dict(), optimizer.state_dict()), path)\n",
        "        checkpoint = tune.Checkpoint.from_directory(temp_checkpoint_dir)\n",
        "        tune.report({**train_metrics, **test_metrics}, checkpoint=checkpoint)\n"
      ],
      "metadata": {
        "id": "Ie4uSwljTmVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataloader: DataLoader, model: NN, loss_fn, optimizer, epoch):\n",
        "    correct = 0\n",
        "    avg_loss = 0\n",
        "\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        output = model(X)\n",
        "        loss = loss_fn(output, y)\n",
        "\n",
        "        # backprop\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # check results\n",
        "        pred_probs = torch.nn.functional.softmax(output, dim=1)\n",
        "        y_pred = pred_probs.argmax(1)\n",
        "        cur_correct = (y_pred == y).sum().item()\n",
        "\n",
        "        correct += cur_correct\n",
        "        avg_loss += loss.item()\n",
        "\n",
        "\n",
        "    correct /= len(dataloader.dataset)  # correct / dataset size\n",
        "    avg_loss /= len(dataloader)         # acc loss / num batches\n",
        "\n",
        "    metrics = { 'train_accuracy': correct, 'train_loss': avg_loss, 'epoch': epoch }\n",
        "\n",
        "    wandb.log(metrics)\n",
        "\n",
        "    return metrics\n"
      ],
      "metadata": {
        "id": "KqSP3u9W0g4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(dataloader: DataLoader, model: NN, loss_fn, epoch):\n",
        "    test_correct = 0\n",
        "    test_loss = 0\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            output = model(X)\n",
        "            loss = loss_fn(output, y)\n",
        "            cur_loss = loss.item()\n",
        "\n",
        "            pred_probs = torch.nn.functional.softmax(output, dim=1)\n",
        "            y_pred = pred_probs.argmax(1)\n",
        "            cur_correct = (y_pred == y).sum().item()\n",
        "\n",
        "            test_correct += cur_correct\n",
        "            test_loss += cur_loss\n",
        "\n",
        "\n",
        "    test_correct /= len(dataloader.dataset)     # correct / dataset size\n",
        "    test_loss /= len(dataloader)                # acc loss / num batches\n",
        "\n",
        "    metrics = { 'test_accuracy': test_correct, 'test_loss': test_loss, 'epoch': epoch }\n",
        "\n",
        "    wandb.log(metrics)\n",
        "\n",
        "    return metrics\n"
      ],
      "metadata": {
        "id": "hcGxkBb5Duxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_best_model(best_result, input):\n",
        "    best_model = NN(\n",
        "        best_result.config['l1'],\n",
        "        best_result.config['l2'],\n",
        "        best_result.config['l1_dropout']\n",
        "    )\n",
        "\n",
        "    best_model.to(device)\n",
        "\n",
        "    checkpoint_path = os.path.join(best_result.checkpoint.to_directory(), 'checkpoint.pt')\n",
        "    model_state, optimizer_state = torch.load(checkpoint_path)\n",
        "    best_model.load_state_dict(model_state)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        input = input.to(device)\n",
        "\n",
        "        output = best_model(input)\n",
        "        pred_probs = torch.nn.functional.softmax(output, dim=1)\n",
        "        y_pred = pred_probs.argmax(1)\n",
        "\n",
        "\n",
        "    submission = [['ImageId', 'Label']]\n",
        "    for i,pred in enumerate(y_pred):\n",
        "        submission.append([i+1, pred.item()])\n",
        "\n",
        "\n",
        "    np.savetxt('submission.csv', submission, delimiter=',', fmt='%s')\n"
      ],
      "metadata": {
        "id": "0HXTFh-WWa6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_tuned_model(config):\n",
        "    lr = config['lr']\n",
        "    epochs = config['epochs']\n",
        "    batch_size = config['batch_size']\n",
        "\n",
        "    model = NN(\n",
        "        config['l1'],\n",
        "        config['l2'],\n",
        "        config['l1_dropout']\n",
        "    )\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    # loss function and optimizer\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "    # test and train data loaders\n",
        "    train_dataloader = DataLoader(ray.get(train_dataset_ref), batch_size=batch_size, shuffle=True)\n",
        "    test_dataloader = DataLoader(ray.get(test_dataset_ref), batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "    # ray tune load checkpoint\n",
        "    raytune_load_checkpoint(model, optimizer)\n",
        "\n",
        "    test_metrics = None\n",
        "    train_metrics = None\n",
        "\n",
        "    # init logging\n",
        "    wandb.init(project='Digit Recognizer', group='experiment_3', config=config)\n",
        "\n",
        "    # train/test iterations\n",
        "    for t in range(epochs):\n",
        "        # print(f'Epoch {t+1}\\n---------------------')\n",
        "        train_metrics = train(train_dataloader, model, loss_fn, optimizer, t)\n",
        "        test_metrics = test(test_dataloader, model, loss_fn, t)\n",
        "\n",
        "\n",
        "    # finish logging\n",
        "    wandb.finish()\n",
        "\n",
        "    # ray tune save checkpoint\n",
        "    raytune_save_checkpoint(model, optimizer, train_metrics, test_metrics)\n",
        "\n",
        "\n",
        "    print('Done!')"
      ],
      "metadata": {
        "id": "epe1tengQVDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    'l1': tune.choice([256,512]),\n",
        "    'l2': tune.choice([32,64,128]),\n",
        "    'l1_dropout': tune.uniform(0, 0.5),\n",
        "    'lr': tune.loguniform(5e-4,1e-3),\n",
        "    'batch_size': tune.choice([32,64,128]),\n",
        "    'epochs': 20,\n",
        "    'num_trials': 30\n",
        "}\n",
        "\n",
        "scheduler = ASHAScheduler(\n",
        "    time_attr='training_iteration',\n",
        "    max_t=config['epochs'],\n",
        "    grace_period=10,\n",
        "    reduction_factor=2\n",
        ")\n",
        "\n",
        "tuner = tune.Tuner(\n",
        "    tune.with_resources(\n",
        "        tune.with_parameters(train_tuned_model),\n",
        "        resources={ 'cpu': 2, 'gpu': 1 }\n",
        "    ),\n",
        "\n",
        "    tune_config=tune.TuneConfig(\n",
        "        metric='test_loss',\n",
        "        mode='min',\n",
        "        scheduler=scheduler,\n",
        "        num_samples=config['num_trials']\n",
        "    ),\n",
        "\n",
        "    param_space=config\n",
        ")\n",
        "\n",
        "results = tuner.fit()\n",
        "\n",
        "best_result = results.get_best_result('test_loss', 'min')\n",
        "\n",
        "print(f'Best trial config: { best_result.config }')\n",
        "print(f'Best trial final validation loss: { best_result.metrics[\"test_loss\"] }')\n",
        "print(f'Best trial final validation accuracy: { best_result.metrics[\"test_accuracy\"] }')\n",
        "\n",
        "test_best_model(best_result, submission_dataset)"
      ],
      "metadata": {
        "id": "oOGtYs1ZaxpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Model visualization](https://wandb.ai/edwinsoftwaredev-personal/Digit%20Recognizer)"
      ],
      "metadata": {
        "id": "VeE0sOiaTMJ0"
      }
    }
  ]
}